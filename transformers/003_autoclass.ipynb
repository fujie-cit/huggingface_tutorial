{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoClassを利用して事前学習済みインスタンスをロードする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**※説明の読みやすさのために，警告を非表示にします．実際のプログラムでは警告の表示を推奨します．**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 警告の表示を停止\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Transformersのログを非表示\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformerをベースにした非常に多様なアーキテクチャが存在するため，目的のタスクに合ったモデルを作成するのが難しい場合があります．\n",
    "Hugging Face Transformersの基本方針として，ライブラリを使いやすく，シンプルで柔軟にするために，`AutoClass`は与えられたチェックポイントから適切なアーキテクチャを自動的に推論し，ロードします．\n",
    "`from_pretrained()`メソッドを使用すると，事前学習済みモデルを素早くロードできるため，モデルをゼロから学習するための時間やリソースを節約することができます．\n",
    "このようにチェックポイントに依存しないコードを生成することは，コードがあるチェックポイントで動作すれば，異なるアーキテクチャのモデルでも同じタスクに向けて学習されていれば利用できることを意味します．\n",
    "\n",
    "> **アーキテクチャ**とはモデルの**構造**を指します．チェックポイントは特定のアーキテクチャの重みです．\n",
    "> 例えば，`BERT`はアーキテクチャであり，`google-bert/bert-base-uncased`はチェックポイントです．\n",
    "> モデルはアーキテクチャまたはチェックポイントのどちらかを指す一般的な用語です．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoTokenizer\n",
    "\n",
    "ほとんどのNLP（自然言語処理）タスクはトークナイザで始まります．\n",
    "トークナイザは入力をモデルで処理できる形式に変換します．\n",
    "\n",
    "`AutoTokenizer.from_pretrained()`を使用してトークナイザをロードします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，トークナイザに入力を渡します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sequence = \"In a hole in the ground there lived a hobbit.\"\n",
    "print(tokenizer(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoImageProcessor\n",
    "\n",
    "ビジョンタスクの場合，画像処理が画像を正しい入力形式に変換します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "\n",
    "# 画像の読み込み\n",
    "# https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg をPIL Imageとして読み込む\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = Image.open(BytesIO(requests.get(image_url).content))\n",
    "\n",
    "# 実際の特長抽出\n",
    "inputs = image_processor(images=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoFeatureExtractor\n",
    "\n",
    "オーディオタスクの場合，特徴量抽出がオーディオ信号を正しい入力形式に変換します．\n",
    "\n",
    "`AutoFeatureExtractor.from_pretrained()`を使用して特徴量抽出器をロードします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    ")\n",
    "\n",
    "# 音声ファイルの読み込み\n",
    "import torchaudio\n",
    "audio_url = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
    "waveform, sample_rate = torchaudio.load(audio_url)\n",
    "\n",
    "# サンプリングレートの調整\n",
    "if sample_rate != feature_extractor.sampling_rate:\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, feature_extractor.sampling_rate)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# 実際の特徴抽出\n",
    "inputs = feature_extractor(waveform, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoProcessor\n",
    "\n",
    "マルチモーダルタスクの場合，2つの前処理ツールを組み合わせることが必要になってきます．\n",
    "例えば，`LayoutLMV2`モデルは画像を処理するための画像処理と，テキストを処理するためのトークナイザが必要です．\n",
    "`AutoProcessor`は，これらの両方を組み合わせます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "inputs = processor(image, \"In a hole in the ground there lived a hobbit.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModel\n",
    "\n",
    "`AutoModelFor???`クラスは特定のタスクに対して事前学習済モデルをロードできます．\n",
    "使用できるモデルの一覧は[こちら](https://huggingface.co/docs/transformers/ja/model_doc/auto)を参照してください．\n",
    "\n",
    "例えば，`AutoModelForSequenceClassification.from_pretrained()`を使用して，シーケンス分類用のモデルをロードできます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じチェックポイントを再利用して異なるタスクのアーキテクチャをロードできます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前学習済みのモデルのインスタンスをロードするのに`AutoTokenizer`クラスと`AutoModelFor?`クラスの使用が推奨されます．\n",
    "これにより，常に正しいアーキテクチャをロードできます．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
