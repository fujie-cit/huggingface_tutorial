{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前学習モデルのファインチューニング\n",
    "\n",
    "事前学習済みモデルを利用することで，計算コストを削減し，CO2排出量を減少させ，ゼロから学習することなく最新のモデルを使用できるという利点があります．\n",
    "Hugging Face Transformers は，さまざまなタスクに対応した数千もの事前学習済みモデルを提供します．\n",
    "事前学習済みモデルを使用する場合，特定のタスクにあわせたデータセットで学習します．\n",
    "これはファインチューニングという呼ばれ，非常に強力な学習技術です．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**※説明の読みやすさのために，警告を非表示にします．実際のプログラムでは警告の表示を推奨します．**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 警告の表示を停止\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Transformersのログを非表示\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの準備\n",
    "\n",
    "事前学習済みのモデルをファインチューニングする前に，データセットをダウンロードしてトレーニング用に準備する必要があります．\n",
    "\n",
    "まず，[Yelp Reviews](https://huggingface.co/datasets/yelp_review_full)データセットを読み込みましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Datasets の`map`メソッドを利用して，データ全体に前処理（トークナイザによるトークン化，パディング/切り詰め）を行います:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095d12867c324161af143aae36c66845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要に応じて，実行時間の短縮のためにフルデータセットの小さなサブセットを作ります:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "実際の学習方法は，利用するフレームワークによって異なります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Trainer を利用した学習\n",
    "\n",
    "Hugging Face Transformers は，提供するモデルの学習に最適化された`Trainer`クラスを提供しています．\n",
    "独自の学習ループを記述することなく，学習を開始しやすくしています．\n",
    "`Trainer API`は，ログの記録，勾配累積，混合精度など，さまざまな学習オプションと機能をサポートしています．\n",
    "\n",
    "まず，モデルをロードし，予想されるラベルの数を指定します．\n",
    "`Yelp Review`の`datset card`から，5つのラベルがあることがわかります:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 一部の学習済みの重みが使用されず，ランダムに初期化されたという警告が表示されることがありますが，これは正常です．\n",
    "> BERTモデルの事前学習済みのヘッドは破棄され，ランダムに初期化された分類ヘッドで置き換えられたためです．\n",
    "> この新しいモデルヘッドをシーケンス分類タスクでファインチューニングするのです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習用のハイパーパラメータ\n",
    "\n",
    "次に，学習オプションを有効にするための全てのハイパーパラメータと，調整できるハイパーパラメータをふくむ`TrainingArguments`クラスを作成します．\n",
    "ここでは，デフォルトの学習ハイパーパラメータを使用しますが，最適な設定を見つけるためにこれらを調整しても構いません．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習のチェックポイントを保存する場所を指定します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価\n",
    "\n",
    "`Trainer`は，学習中に自動的にモデルの性能を評価しません．\n",
    "評価値を計算して報告する関数を`Trainer`に渡す必要があります．\n",
    "Hugging Face Evaluateライブラリでは，`evaluate.load`関数を使用して読み込むことができるシンプルな`accuracy`関数を提供しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`metric`の`~evaluate.compute`を呼び出して，予測の正確度を計算します．\n",
    "`compute`に予測を渡す前に，予測のロジットに変換する必要があります．\n",
    "（すべてのHugging Face Transformesモデルはロジットを返します）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価値をファインチューニング中に監視したい場合，学習の引数で`eval_strategy`パラメータを指定して，各エポックの終了時に評価値を報告させます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "モデル，学習のための引数，学習およびデータセット，評価関数を使用して`Trainer`オブジェクトを作成します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その後，`train()`を呼び出してファインチューニングを実行します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0938891172409058, 'eval_accuracy': 0.551, 'eval_runtime': 3.1884, 'eval_samples_per_second': 313.634, 'eval_steps_per_second': 39.204, 'epoch': 1.0}\n",
      "{'eval_loss': 1.0334631204605103, 'eval_accuracy': 0.541, 'eval_runtime': 3.1999, 'eval_samples_per_second': 312.508, 'eval_steps_per_second': 39.063, 'epoch': 2.0}\n",
      "{'eval_loss': 0.986595869064331, 'eval_accuracy': 0.601, 'eval_runtime': 3.2094, 'eval_samples_per_second': 311.584, 'eval_steps_per_second': 38.948, 'epoch': 3.0}\n",
      "{'train_runtime': 41.8487, 'train_samples_per_second': 71.687, 'train_steps_per_second': 8.961, 'train_loss': 1.0666346842447916, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.0666346842447916, metrics={'train_runtime': 41.8487, 'train_samples_per_second': 71.687, 'train_steps_per_second': 8.961, 'train_loss': 1.0666346842447916, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native PyTorch での学習\n",
    "\n",
    "(割愛)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追加情報\n",
    "\n",
    "- [Hugging Face Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples)には，PyTorchとTensorFlowで一般的なNLPタスクを学習するスクリプトが含まれています．\n",
    "- [Hugging Face Transformers Notebooks](https://huggingface.co/docs/transformers/ja/notebooks)には，特定のタスクにモデルをファインチューニングする方法に関するさまざまなノートブックが含まれています．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
