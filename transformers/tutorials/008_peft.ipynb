{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face PEFT を利用したアダプタの学習\n",
    "\n",
    "Parameter-Efficient Fine Tuning (PEFT)メソッドは，事前学習済みモデルのパラメタをファインチューニング中にフリーズし，その上に学習可能なパラメータ（アダプタ）を追加する方法です．\n",
    "アダプタは，タスク固有の情報を効率よく学習することが期待されます．\n",
    "この方法は，メモリ使用量が少なく，完全にファインチューニングされたモデルと比較して計算リソースを低く抑えつつ，同等の結果が得られることが示されています．\n",
    "\n",
    "PEFTで学習されたアダプタは，完全なモデルのサイズよりも1桁小さく，共有，保存，読み込むのが便利です．\n",
    "\n",
    "> Hubに格納されている OPTForCausalLM モデルのアダプタの重みは約6MBで，モデルの全体の重みのサイズ約700MBに対して1%程度のサイズです．\n",
    "\n",
    "Hugging Face PEFTについて詳しく知りたい場合は[ドキュメント](https://huggingface.co/docs/peft/index)を参照してください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール\n",
    "\n",
    "Hugging Face PEFT をインストールして始めましょう:\n",
    "```\n",
    "pip install peft\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サポート対象のPEFTモデル\n",
    "\n",
    "Hugging Face Transformers は，いくつかのPEFT（Parameter Efficient Fine-Tuning）メソッドをネイティブにサポートしており，ローカルまたはHubに格納されたアダプタウェイトを簡単に読み込んで実行または学習することができます．\n",
    "以下の手法がサポートされています．\n",
    "\n",
    "- Low Rank Adapters (LoRA)\n",
    "- IA3\n",
    "- AdaLoRA\n",
    "\n",
    "他の方法を使用したい場合，プロンプト学習やプロンプト調整などについて詳しく知りたい場合，または hugging Face PEFT ライブラリ全般については，[ドキュメント](https://huggingface.co/docs/peft/index)を参照してください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFTアダプタの読み込み\n",
    "\n",
    "Hugging Face TransformersからPEFTアダプタモデルを読み込んで使用するには，Hubリポジトリまたはローカルディレクトリにadapter_config.jsonファイルとアダプタウェイトが含まれていることを確認してください．\n",
    "次に，`AutoModelFor?`クラスを使用してPEFTアダプタモデルを読み込むことができます．\n",
    "例えば，因果的言語モデル用のアダプタを読み込むには:\n",
    "\n",
    "1. PEFTモデルのIDを指定します\n",
    "2. それを`AutoModelForCausalLM`クラスに渡します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py311/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "mdoel = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PEFTアダプタを`AutoModelFor?`クラスまたは基本モデルクラス（`OptForCausalLM`または`LlamaForCausalLM`など）で読み込むことができます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，`load_adapter`メソッドを呼び出すことで，PEFTアダプタを読み込むこともできます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "mdoel.load_adapter(peft_model_id, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8bitまたは4bitでのロード\n",
    "\n",
    "`bitsandbytes`との統合により，PEFTアダプタを8ビットまたは4ビットの精度で読み込むことができます．\n",
    "これにより，大規模なモデルを読み込む際にメモリを節約することができます（[詳細](https://huggingface.co/docs/transformers/ja/quantization#bitsandbytes-integration)）\n",
    "`from_pretrained()`の`quontization_config`パラメータで設定を変更し，`device=\"auto\"`を指定することで，自動的に最適なデバイスにアダプタを配置することができます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "mdoel = AutoModelForCausalLM.from_pretrained(peft_model_id,\n",
    "                                             config=BitsAndBytesConfig(load_in_8bit=True), # Load the model in 8-bit\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新しいアダプタを追加する\n",
    "\n",
    "既存のアダプタを持つモデルに新しいアダプタを追加するために，`add_adapter`メソッドを使用することができます．\n",
    "ただし，新しいアダプタは既存のアダプタと同じタイプである必要があります．\n",
    "例えば，既存のモデルにLoRAアダプタがアタッチされている場合:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False,\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しいアダプタを追加するには:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach new adapter with same config\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_adapter`メソッドを使用して，どのアダプタを使用するかを指定します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_adapter_1\n",
    "model.set_adapter(\"adapter_1\")\n",
    "output = model.generate(**inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
