{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livedoor News Corpus ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livedoor News Corpus ã®æº–å‚™\n",
    "\n",
    "### Livedoor News Corpusã¨ã¯\n",
    "\n",
    "[Livedoor News Corpus](https://www.rondhuit.com/download.html#news%20corpus)ã¯ã€ãƒ©ã‚¤ãƒ–ãƒ‰ã‚¢ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¨˜äº‹ã‚’åˆ©ç”¨ã—ã¦ä½œæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ï¼Œä»¥ä¸‹ã®æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼\n",
    "- è¨˜äº‹ã®URL\n",
    "- è¨˜äº‹ã®æ—¥ä»˜\n",
    "- è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«\n",
    "- è¨˜äº‹ã®æœ¬æ–‡\n",
    "- è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒª\n",
    "\n",
    "è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªã¯ï¼Œ\"ãƒˆãƒ”ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹\"ï¼Œ\"Sports Watch\"ï¼Œ\"ITãƒ©ã‚¤ãƒ•ãƒãƒƒã‚¯\"ï¼Œ\"å®¶é›»ãƒãƒ£ãƒ³ãƒãƒ«\"ï¼Œ\"MOVIE ENTER\"ï¼Œ\"ç‹¬å¥³é€šä¿¡\"ï¼Œ\"ã‚¨ã‚¹ãƒãƒƒã‚¯ã‚¹ï¼Œ\"livedoor HOMME\"ï¼Œ\"Peachy\"ã®9ã¤ã‹ã‚‰ãªã‚Šã¾ã™ï¼\n",
    "\n",
    "è¨˜äº‹ã¯å…¨éƒ¨ã§7,367ä»¶ã‚ã‚Šã¾ã™ï¼\n",
    "\n",
    "å…ƒã€…ã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ãŒï¼ŒHugging Face Datasetsã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼Œä»Šå›ã¯ã“ã‚Œã‚’åˆ©ç”¨ã—ã¾ã™ï¼\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ç¢ºèª\n",
    "\n",
    "Hugging Face Datasetsã‚’åˆ©ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"shunk031/livedoor-news-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã¯ï¼Œ`train`ï¼Œ`validation`ï¼Œ`test`ã®3ã¤ã«åˆ†ã‹ã‚Œã¦ã„ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "# datasetã®ã‚­ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã®ä¸­èº«ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 3,\n",
      " 'content': 'NHKã®æƒ…å ±ç•ªçµ„ã€ŒãŠå…ƒæ°—ã§ã™ã‹æ—¥æœ¬åˆ—å³¶ã€å†…ã®ã€Œã“ã¨ã°ãŠã˜ã•ã‚“ã®æ°—ã«ãªã‚‹ã“ã¨ã°ã€ã¯ã€æ¯å›ã€è¨€è‘‰ã®ç–‘å•ã«è¿«ã£ã¦ã„ãã‚³ãƒ¼ãƒŠãƒ¼ã ãŒã€24æ—¥ã«æ”¾é€ã•ã‚ŒãŸã€Œæ—¥æœ¬ã«æµ¸é€ã—ã¦ã„ã‚‹éŸ“å›½èªã€ã®å†…å®¹ãŒã€ãƒãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–“ã§æ³¨ç›®ã‚’é›†ã‚ã¦ã„ã‚‹ã€‚  '\n",
      "            'æ”¾é€ã«ã‚ˆã‚‹ã¨ã€ã„ã¾æ—¥æœ¬ã®è‹¥è€…ã®é–“ã§ã¯ã€æºå¸¯ãƒ¡ãƒ¼ãƒ«ã§ãƒãƒ³ã‚°ãƒ«ã®çµµæ–‡å­—ã‚’ä½¿ã†ã®ãŒãƒ–ãƒ¼ãƒ ã ã¨ä¼ãˆã¦ã„ã‚‹ã€‚ã€Œãƒãƒ³ã‚°ãƒ«ã¯ã‹ã‚ã„ãã¦ãƒ‡ã‚¶ã‚¤ãƒ³ã«ã—ã‚„ã™ã„ã€ã¨æ„Ÿã˜ã‚‹äººãŒå¢—ãˆã¦ã„ã‚‹ãã†ã ã€‚ã¾ãŸã€è‹¥è€…ã¸ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§ã‚‚ã€ŒéŸ“å›½èªã®ã»ã†ãŒç´ ç›´ã«è¨€ãˆã‚‹ã€‚æ—¥æœ¬èªã ã¨æ¥ãšã‹ã—ã„ã€ã€Œæ—¥æœ¬èªã§ã¯è¬ã‚Šã«ãã„ãŒã€ãƒŸã‚¢ãƒãƒ¨ã€ã‚ªãƒ³ãƒã€ï¼ˆã”ã‚ã‚“ã­ã€ãƒãƒï¼‰ã ã¨è¨€ã„ã‚„ã™ã„ã€ã¨ç­”ãˆã¦ãŠã‚Šã€å®Ÿéš›ã«ãƒãƒ³ã‚°ãƒ«ã‚’ä½¿ã£ãŸãƒ¡ãƒ¼ãƒ«ã‚‚ç´¹ä»‹ã•ã‚ŒãŸã€‚  '\n",
      "            'ã“ã‚Œã«å¯¾ã—ã¦ãƒãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œãã‚“ãªè©±èã„ãŸã“ã¨ãªã„ã€ã€Œã“ã‚“ãªãƒ¡ãƒ¼ãƒ«æ¥ãŸã‚‰ç¸ã‚’åˆ‡ã‚‹ã‚ã€ãªã©ã€ç•ªçµ„ãŒç‰¹é›†ã—ãŸâ€œãƒ–ãƒ¼ãƒ â€ã®å­˜åœ¨ã«ç–‘å•ã‚’å‘ˆã™ã‚‹å£°ãŒç¶šå‡ºã€‚ã¾ãŸã€ã€Œãƒ•ã‚¸ã‹ã¨æ€ã£ãŸã‚‰NHKã‹ã‚ˆã€ã€Œå—ä¿¡æ–™æ‰•ã„ãŸããªã„ã€ã€Œä»Šåº¦ã¯NHKãƒ‡ãƒ¢ã‹?wwã€ãªã©ã€NHKãŒéŸ“å›½å¯„ã‚Šã®ç•ªçµ„ã‚’æ”¾é€ã—ã¦ã„ãŸã“ã¨ã«æ‰¹åˆ¤çš„ãªãƒãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å£°ã‚‚ç›®ç«‹ã£ãŸã€‚  '\n",
      "            'ã€é–¢é€£æƒ…å ±ã€‘ ãƒ»ã€Œã“ã¨ã°ãŠã˜ã•ã‚“ã®æ°—ã«ãªã‚‹ã“ã¨ã°ã€  ãƒ»ä»Šã€æ—¥æœ¬èªã«ã•ã‚Šã’ãªãéŸ“å›½èªã‚’æ··ãœã‚‹ã®ãŒå¤§ãƒ–ãƒ¼ãƒ ',\n",
      " 'date': '2011-11-25T19:48:00+0900',\n",
      " 'title': 'NHKã®â€³éŸ“æµå¯„ã‚Šâ€³ç•ªçµ„ã«æ‰¹åˆ¤ã®å£°',\n",
      " 'url': 'http://news.livedoor.com/article/detail/6062449/'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã®çµæœã‹ã‚‰ï¼Œãƒ‡ãƒ¼ã‚¿ã¯\n",
    "- 'url' : è¨˜äº‹ã®URL\n",
    "- 'date' : è¨˜äº‹ã®æ—¥ä»˜\n",
    "- 'title' : è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«\n",
    "- 'content' : è¨˜äº‹ã®æœ¬æ–‡\n",
    "- 'category' : è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒª\n",
    "ã§ä¸ãˆã‚‰ã‚Œã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train`, `valudation`, `test` ãã‚Œãã‚Œã§ã‚«ãƒ†ã‚´ãƒªã®åˆ†å¸ƒã‚’ç¢ºèªã—ã¦ã¿ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Counter({6: 900, 0: 870, 1: 870, 8: 870, 5: 842, 2: 772, 3: 770})\n",
      "validation: Counter({4: 511, 7: 134, 2: 92})\n",
      "test: Counter({7: 736})\n"
     ]
    }
   ],
   "source": [
    "# trainã®categoryã®åˆ†å¸ƒã‚’è¨ˆç®—ã™ã‚‹\n",
    "from collections import Counter\n",
    "\n",
    "for set_name in dataset.keys():\n",
    "    category_counter = Counter([data[\"category\"] for data in dataset[set_name]])\n",
    "    print(f\"{set_name}: {category_counter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã®ãƒ©ãƒ™ãƒ«ã®ãƒãƒ©ãƒ³ã‚¹ã§ã¯é©åˆ‡ãªå­¦ç¿’ãŒã§ããªã„ãŸã‚ï¼Œæ”¹ã‚ã¦ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚ŠãªãŠã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5893\n",
      "valid: 737\n",
      "test: 737\n"
     ]
    }
   ],
   "source": [
    "# datasetã®ä¸­èº«ã‚’å…¨ã¦çµ±åˆã™ã‚‹\n",
    "all_data = []\n",
    "for set_name in dataset.keys():\n",
    "    all_data.extend(dataset[set_name])\n",
    "# ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—, 80%ã‚’train, 10%ã‚’validation, 10%ã‚’testã«ã™ã‚‹\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(all_data)\n",
    "train_data = all_data[:int(0.8 * len(all_data))]\n",
    "valid_data = all_data[int(0.8 * len(all_data)):int(0.9 * len(all_data))]\n",
    "test_data = all_data[int(0.9 * len(all_data)):]\n",
    "print(f\"train: {len(train_data)}\")\n",
    "print(f\"valid: {len(valid_data)}\")\n",
    "print(f\"test: {len(test_data)}\")\n",
    "\n",
    "# æ”¹ã‚ã¦ï¼ŒDatasets ã®å½¢å¼ã«ã™ã‚‹\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_train = Dataset.from_list(train_data, features=dataset[\"train\"].features)\n",
    "dataset_valid = Dataset.from_list(valid_data, features=dataset[\"train\"].features)\n",
    "dataset_test = Dataset.from_list(test_data, features=dataset[\"train\"].features)\n",
    "dataset[\"train\"] = dataset_train\n",
    "dataset[\"validation\"] = dataset_valid\n",
    "dataset[\"test\"] = dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Counter({6: 735, 0: 712, 7: 699, 8: 696, 2: 678, 5: 670, 1: 668, 3: 614, 4: 421})\n",
      "validation: Counter({1: 102, 8: 98, 7: 94, 2: 88, 5: 87, 6: 83, 3: 72, 0: 71, 4: 42})\n",
      "test: Counter({1: 100, 2: 98, 0: 87, 5: 85, 3: 84, 6: 82, 7: 77, 8: 76, 4: 48})\n"
     ]
    }
   ],
   "source": [
    "# å†åº¦ãƒãƒ©ãƒ³ã‚¹ã‚’ç¢ºèª\n",
    "for set_name in dataset.keys():\n",
    "    category_counter = Counter([data[\"category\"] for data in dataset[set_name]])\n",
    "    print(f\"{set_name}: {category_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†…å®¹ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å¤‰æ›´ã—ã¾ã™:\n",
    "- å…¥åŠ›ã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ãŸã‚ã«ï¼Œè¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’é€£çµã—ï¼Œ'text'ã¨ã„ã†ã‚­ãƒ¼ã§ä¿å­˜ã—ã¾ã™ï¼\n",
    "- å‡ºåŠ›ãƒ©ãƒ™ãƒ«ã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ãŸã‚ã«ï¼Œã‚«ãƒ†ã‚´ãƒªã‚’'label'ã¨ã„ã†ã‚­ãƒ¼ã§ä¿å­˜ã—ã¾ã™ï¼ãªãŠï¼Œã‚«ãƒ†ã‚´ãƒªã¯ã™ã§ã«æ•°å€¤ã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼Œãã®ã¾ã¾åˆ©ç”¨ã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a1768f99804b7e90883019329b74b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac1f523e4ef4a0db256c78c7920610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7032f0dc425479a900d15d677dc005a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    return {\"text\": example[\"title\"] + \"ï¼š\" + example[\"content\"],\n",
    "            \"label\": example[\"category\"]}\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é©åˆ‡ã«å¤‰æ›ã•ã‚ŒãŸã‹ç¢ºã‹ã‚ã¦ã¿ã¾ã—ã‚‡ã†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 6,\n",
      " 'content': 'å»ã‚‹9æœˆ18æ—¥ã«ãƒœãƒ¼ãƒˆã®å…¨æ—¥æœ¬é¸æ‰‹æ¨©ãŒè¡Œã‚ã‚Œã€å¥³å­ã‚·ãƒ³ã‚°ãƒ«ã‚¹ã‚«ãƒ«æ±ºå‹ã§è‹¥äº•æ±Ÿåˆ©ãŒéŠ€ãƒ¡ãƒ€ãƒ«ã‚’ç²å¾—ã—ãŸã€‚  '\n",
      "            'å··ã§ã¯ãã‚Œã»ã©åã®çŸ¥ã‚ŒãŸå­˜åœ¨ã§ã¯ãªã„ãŒã€è‹¥äº•ã¯çŸ¥ã‚‹äººãçŸ¥ã‚‹ç¾äººã‚¢ã‚¹ãƒªãƒ¼ãƒˆã€‚ç«¶æŠ€ã§æ—¥ç„¼ã‘ã—ãŸå°éº¦è‰²ã®è‚Œã«ã€ã•ã‚ã‚„ã‹ãªç¬‘é¡”ã‚’æŒ¯ã‚Šã¾ãã€ãƒœãƒ¼ãƒˆç•Œã§ã¯éšä¸€ã®ç¾äººé¸æ‰‹ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚  '\n",
      "            'ã‚¹ãƒãƒ¼ãƒ„ã‚¿ãƒ³ã‚¯ãƒˆãƒƒãƒ—ã«ã‚µãƒ³ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‹ã‘ã‚‹æ§˜ã¯ã€ã“ã‚Œãå¥åº·ç¾äººã¨ã„ã†è¶£ã ã€‚  '\n",
      "            'è‹¥äº•ã¯ãƒœãƒ¼ãƒˆã§æœ‰åãªå²é˜œçœŒã®åŠ èŒ‚é«˜æ ¡å‡ºèº«ã€‚é«˜æ ¡åœ¨å­¦ä¸­ã«ç·ä½“ãƒ€ãƒ–ãƒ«ã‚¹ã‚«ãƒ«ã§å„ªå‹ã—ã€ã‚¸ãƒ¥ãƒ‹ã‚¢é¸æ‰‹æ¨©æ—¥æœ¬ä»£è¡¨ã§ã‚‚æ´»èºã€‚é€²å­¦ã—ãŸæ—©ç¨²ç”°å¤§ã§ã‚‚æ•°ã€…ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç²å¾—ã—ã€2006å¹´ã®ã‚¢ã‚¸ã‚¢å¤§ä¼šã§ã¯æ—¥æœ¬ä»£è¡¨ã¨ã—ã¦éŠ€ãƒ¡ãƒ€ãƒ«ã‚’æ‰‹ã«ã—ãŸã€‚  '\n",
      "            'ç¾åœ¨ã¯ä¼æ¥­ã‹ã‚‰ã‚¹ãƒãƒ³ã‚µãƒ¼ãƒ‰ã‚’å—ã‘ãªãŒã‚‰ãƒ•ãƒ«ã‚¿ã‚¤ãƒ ã®ãƒœãƒ¼ãƒˆé¸æ‰‹ã¨ã—ã¦æ´»å‹•ã—ã€2010å¹´ã‚¢ã‚¸ã‚¢é¸æ‰‹æ¨©ã§ã¯è¦‹äº‹å„ªå‹ã‚’æœãŸã™ãªã©ã€æ—¥æœ¬ã®å¥³å­ãƒœãƒ¼ãƒˆç•Œã‚’ç‰½å¼•ã™ã‚‹ã‚¢ã‚¹ãƒªãƒ¼ãƒˆã¨ã—ã¦æœŸå¾…ã•ã‚Œã¦ã„ã‚‹ã€‚  '\n",
      "            'ä»Šå›ã®å…¨æ—¥æœ¬é¸æ‰‹æ¨©ã§ã®2ä½ã¨ã„ã†çµæœã«ã¤ã„ã¦ã¯ã€è‡ªèº«ã®ãƒ–ãƒ­ã‚°ã§ã€Œä¸€ç•ªå«Œã„ãªè‰²ã®ãƒ¡ãƒ€ãƒ«ã§ã™ãŒã€ç¾å®Ÿã‚’ã—ã£ã‹ã‚Šå—ã‘æ­¢ã‚ã¦ã€æ¬¡ã¸é€²ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€ã¨ã‚³ãƒ¡ãƒ³ãƒˆã€‚ç›®æ¨™ã¨ã™ã‚‹ãƒ­ãƒ³ãƒ‰ãƒ³äº”è¼ªã§ã®è¡¨å½°å°ã¸å‘ã‘ã€å…¨é€ŸåŠ›ã§æ°´é¢ã‚’é§†ã‘ã‚‹ã€‚  '\n",
      "            'ãƒ»è‹¥äº•æ±Ÿåˆ©ãƒ•ã‚©ãƒˆã‚®ãƒ£ãƒ©ãƒªãƒ¼',\n",
      " 'date': '2011-09-27T08:30:00+0900',\n",
      " 'label': 6,\n",
      " 'text': 'ã€Sports '\n",
      "         'Watchã€‘çŸ¥ã‚‹äººãçŸ¥ã‚‹ç¾äººã‚¢ã‚¹ãƒªãƒ¼ãƒˆã€å°éº¦è‚Œã®æ¼•è‰‡é¸æ‰‹ï¼šå»ã‚‹9æœˆ18æ—¥ã«ãƒœãƒ¼ãƒˆã®å…¨æ—¥æœ¬é¸æ‰‹æ¨©ãŒè¡Œã‚ã‚Œã€å¥³å­ã‚·ãƒ³ã‚°ãƒ«ã‚¹ã‚«ãƒ«æ±ºå‹ã§è‹¥äº•æ±Ÿåˆ©ãŒéŠ€ãƒ¡ãƒ€ãƒ«ã‚’ç²å¾—ã—ãŸã€‚  '\n",
      "         'å··ã§ã¯ãã‚Œã»ã©åã®çŸ¥ã‚ŒãŸå­˜åœ¨ã§ã¯ãªã„ãŒã€è‹¥äº•ã¯çŸ¥ã‚‹äººãçŸ¥ã‚‹ç¾äººã‚¢ã‚¹ãƒªãƒ¼ãƒˆã€‚ç«¶æŠ€ã§æ—¥ç„¼ã‘ã—ãŸå°éº¦è‰²ã®è‚Œã«ã€ã•ã‚ã‚„ã‹ãªç¬‘é¡”ã‚’æŒ¯ã‚Šã¾ãã€ãƒœãƒ¼ãƒˆç•Œã§ã¯éšä¸€ã®ç¾äººé¸æ‰‹ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚  '\n",
      "         'ã‚¹ãƒãƒ¼ãƒ„ã‚¿ãƒ³ã‚¯ãƒˆãƒƒãƒ—ã«ã‚µãƒ³ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‹ã‘ã‚‹æ§˜ã¯ã€ã“ã‚Œãå¥åº·ç¾äººã¨ã„ã†è¶£ã ã€‚  '\n",
      "         'è‹¥äº•ã¯ãƒœãƒ¼ãƒˆã§æœ‰åãªå²é˜œçœŒã®åŠ èŒ‚é«˜æ ¡å‡ºèº«ã€‚é«˜æ ¡åœ¨å­¦ä¸­ã«ç·ä½“ãƒ€ãƒ–ãƒ«ã‚¹ã‚«ãƒ«ã§å„ªå‹ã—ã€ã‚¸ãƒ¥ãƒ‹ã‚¢é¸æ‰‹æ¨©æ—¥æœ¬ä»£è¡¨ã§ã‚‚æ´»èºã€‚é€²å­¦ã—ãŸæ—©ç¨²ç”°å¤§ã§ã‚‚æ•°ã€…ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç²å¾—ã—ã€2006å¹´ã®ã‚¢ã‚¸ã‚¢å¤§ä¼šã§ã¯æ—¥æœ¬ä»£è¡¨ã¨ã—ã¦éŠ€ãƒ¡ãƒ€ãƒ«ã‚’æ‰‹ã«ã—ãŸã€‚  '\n",
      "         'ç¾åœ¨ã¯ä¼æ¥­ã‹ã‚‰ã‚¹ãƒãƒ³ã‚µãƒ¼ãƒ‰ã‚’å—ã‘ãªãŒã‚‰ãƒ•ãƒ«ã‚¿ã‚¤ãƒ ã®ãƒœãƒ¼ãƒˆé¸æ‰‹ã¨ã—ã¦æ´»å‹•ã—ã€2010å¹´ã‚¢ã‚¸ã‚¢é¸æ‰‹æ¨©ã§ã¯è¦‹äº‹å„ªå‹ã‚’æœãŸã™ãªã©ã€æ—¥æœ¬ã®å¥³å­ãƒœãƒ¼ãƒˆç•Œã‚’ç‰½å¼•ã™ã‚‹ã‚¢ã‚¹ãƒªãƒ¼ãƒˆã¨ã—ã¦æœŸå¾…ã•ã‚Œã¦ã„ã‚‹ã€‚  '\n",
      "         'ä»Šå›ã®å…¨æ—¥æœ¬é¸æ‰‹æ¨©ã§ã®2ä½ã¨ã„ã†çµæœã«ã¤ã„ã¦ã¯ã€è‡ªèº«ã®ãƒ–ãƒ­ã‚°ã§ã€Œä¸€ç•ªå«Œã„ãªè‰²ã®ãƒ¡ãƒ€ãƒ«ã§ã™ãŒã€ç¾å®Ÿã‚’ã—ã£ã‹ã‚Šå—ã‘æ­¢ã‚ã¦ã€æ¬¡ã¸é€²ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€ã¨ã‚³ãƒ¡ãƒ³ãƒˆã€‚ç›®æ¨™ã¨ã™ã‚‹ãƒ­ãƒ³ãƒ‰ãƒ³äº”è¼ªã§ã®è¡¨å½°å°ã¸å‘ã‘ã€å…¨é€ŸåŠ›ã§æ°´é¢ã‚’é§†ã‘ã‚‹ã€‚  '\n",
      "         'ãƒ»è‹¥äº•æ±Ÿåˆ©ãƒ•ã‚©ãƒˆã‚®ãƒ£ãƒ©ãƒªãƒ¼',\n",
      " 'title': 'ã€Sports Watchã€‘çŸ¥ã‚‹äººãçŸ¥ã‚‹ç¾äººã‚¢ã‚¹ãƒªãƒ¼ãƒˆã€å°éº¦è‚Œã®æ¼•è‰‡é¸æ‰‹',\n",
      " 'url': 'http://news.livedoor.com/article/detail/5890429/'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®æº–å‚™\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã«ã¯ï¼Œæ±åŒ—å¤§å­¦ãŒæä¾›ã—ã¦ã„ã‚‹[æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«](https://huggingface.co/cl-tohoku/bert-base-japanese-v3)ã‚’åˆ©ç”¨ã—ã¾ã™ï¼\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "ãƒ©ãƒ™ãƒ«æ•°ãŒ9ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ï¼Œ`AutoModelForSequenceClassification`ã¨`AutoTokenizer`ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: i for i, label in enumerate(dataset[\"train\"].features[\"category\"].names)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "num_categories = 9\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                           num_labels=num_categories,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‰å‡¦ç†ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼‰\n",
    "\n",
    "å­¦ç¿’ã‚’ã™ã‚‹ãŸã‚ã«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7ff85fe9dc60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3986c666dcf24708b3ba8718a6f816d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655fb0988cf443b5907802a63f0f8dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7e8f727696443fba5eecc00b9988f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['url',\n",
      " 'date',\n",
      " 'title',\n",
      " 'content',\n",
      " 'category',\n",
      " 'text',\n",
      " 'label',\n",
      " 'input_ids',\n",
      " 'token_type_ids',\n",
      " 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(tokenized_dataset[\"train\"][0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é•·ããªã‚‹ã®ã§å€¤ã®è¡¨ç¤ºã¯å‰²æ„›ã—ã¾ã™ãŒï¼Œ`input_id`ï¼Œ`attention_mask`ï¼Œ`token_type_ids`ã®3ã¤ã®ã‚­ãƒ¼ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å­¦ç¿’\n",
    "\n",
    "### è©•ä¾¡é–¢æ•°\n",
    "\n",
    "å­¦ç¿’è‡ªä½“ã¯`Trainer`ã«ã‚ˆã£ã¦æå¤±é–¢æ•°ãŒè‡ªå‹•çš„ã«è¨ˆç®—ã•ã‚Œã¦å®Ÿè¡Œã•ã‚Œã¾ã™ãŒï¼Œ\n",
    "ã©ã®ç¨‹åº¦ã®æ€§èƒ½ãŒå‡ºã¦ã„ã‚‹ã‹ã¯åˆ¥é€”è©•ä¾¡ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼\n",
    "ãã®ãŸã‚ã®è©•ä¾¡é–¢æ•°ã‚’ç”¨æ„ã—ã¦ãŠãã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ç¿’ã®è¨­å®š\n",
    "\n",
    "å­¦ç¿’ã®è¨­å®šï¼ˆ`TrainingArguments`ï¼‰ã‚’æº–å‚™ã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"base\"\n",
    "output_dir = f\"exp/{exp_name}/results\"\n",
    "logging_dir = f\"exp/{exp_name}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py311/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=10,            # æœ€å¤§10ã‚¨ãƒãƒƒã‚¯ã¨ã™ã‚‹\n",
    "    per_device_train_batch_size=8,  # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    # auto_find_batch_size=True,    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è‡ªå‹•ã§è¦‹ã¤ã‘ã‚‹\n",
    "\n",
    "    weight_decay=0.01,              # é‡ã¿æ¸›è¡°\n",
    "    learning_rate=2e-5,             # å­¦ç¿’ç‡\n",
    "    warmup_steps=500,               # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "\n",
    "    evaluation_strategy=\"epoch\",    # è©•ä¾¡ã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è¡Œã†\n",
    "    metric_for_best_model=\"accuracy\", # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™\n",
    "    greater_is_better=True,           # è©•ä¾¡æŒ‡æ¨™ãŒå¤§ãã„ã»ã©è‰¯ã„å ´åˆã¯True\n",
    "\n",
    "    output_dir=output_dir,          # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆ\n",
    "    save_strategy=\"epoch\",          # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è¡Œã†\n",
    "    save_total_limit=3,             # ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ•°)\n",
    "\n",
    "    logging_dir=logging_dir,        # ãƒ­ã‚°ã®ä¿å­˜å…ˆ\n",
    "    logging_strategy=\"steps\",       # ãƒ­ã‚°ã®ä¿å­˜ã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è¡Œã†\n",
    "    logging_steps=100,              # 100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹\n",
    "\n",
    "    load_best_model_at_end=True,    # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’æœ€å¾Œã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ—©æœŸçµ‚äº†ç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "\n",
    "å­¦ç¿’ã‚’é€”ä¸­ã§çµ‚äº†ã•ã›ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç”¨æ„ã—ã¾ã™ï¼\n",
    "ã“ã“ã§ã¯ï¼Œ3ã‚¨ãƒãƒƒã‚¯ä»¥ä¸Šï¼ŒAccuracyãŒå‘ä¸Šã—ãªã„å ´åˆã«å­¦ç¿’ã‚’çµ‚äº†ã•ã›ã‚‹ã‚ˆã†ã«è¨­å®šã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3, \n",
    "    early_stopping_threshold=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainerã®ä½œæˆ\n",
    "\n",
    "å­¦ç¿’ã‚’è¡Œã†ãŸã‚ã®Trainerã‚’ä½œæˆã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ç¿’ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7370' max='7370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7370/7370 11:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.196954</td>\n",
       "      <td>0.945726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.149820</td>\n",
       "      <td>0.957938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>0.959294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.252548</td>\n",
       "      <td>0.960651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.193202</td>\n",
       "      <td>0.966079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.206794</td>\n",
       "      <td>0.967436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.203992</td>\n",
       "      <td>0.970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.215309</td>\n",
       "      <td>0.971506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.209539</td>\n",
       "      <td>0.972863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.972863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7370, training_loss=0.10265099366778051, metrics={'train_runtime': 677.6127, 'train_samples_per_second': 86.967, 'train_steps_per_second': 10.876, 'total_flos': 1.550610899278848e+16, 'train_loss': 0.10265099366778051, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"exp/{exp_name}/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 97.3%\n"
     ]
    }
   ],
   "source": [
    "# dataset[\"test\"]ã§è©•ä¾¡\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"text\"])\n",
    "# çµæœã®ãƒ©ãƒ™ãƒ«ã‚’ç•ªå·ã®ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "predicted_labels = [label2id[result[\"label\"]] for result in results]\n",
    "\n",
    "# Accuracyã®è¨ˆç®—\n",
    "metrics = metric.compute(predictions=predicted_labels, references=dataset[\"test\"][\"label\"])\n",
    "\n",
    "print(f\"Test set accuracy: {metrics['accuracy']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ˜ãƒƒãƒ‰éƒ¨ã®ã¿ã®å­¦ç¿’\n",
    "\n",
    "å‰ã®ä¾‹ã§ã¯ï¼ŒBERTã®å…¨ä½“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸï¼\n",
    "éå­¦ç¿’ã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã¾ã‚‹ãŸã‚ï¼Œãƒ˜ãƒƒãƒ‰éƒ¨ã®ã¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«ã—ã¾ã™ï¼\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«ã®å†èª­ã¿è¾¼ã¿\n",
    "\n",
    "ã¾ãšï¼Œãƒ¢ãƒ‡ãƒ«ã®èª­ã¿ç›´ã—ã‚’è¡Œã„ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ•ãƒªãƒ¼ã‚º\n",
    "\n",
    "æ¬¡ã«ï¼Œãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ãƒªãƒ¼ã‚ºã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ãƒªãƒ¼ã‚º\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainerã®ä½œæˆ\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆã®ã¿ã‚’å¤‰æ›´ã—ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"head\"\n",
    "training_args.output_dir = f\"exp/{exp_name}/results\"\n",
    "training_args.logging_dir = f\"exp/{exp_name}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ç¿’ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7370' max='7370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7370/7370 04:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.027200</td>\n",
       "      <td>1.954051</td>\n",
       "      <td>0.413840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.752400</td>\n",
       "      <td>1.691338</td>\n",
       "      <td>0.624152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.548100</td>\n",
       "      <td>1.506456</td>\n",
       "      <td>0.687924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.392200</td>\n",
       "      <td>1.371511</td>\n",
       "      <td>0.715061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.307200</td>\n",
       "      <td>1.278642</td>\n",
       "      <td>0.724559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.216000</td>\n",
       "      <td>1.214706</td>\n",
       "      <td>0.724559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.216100</td>\n",
       "      <td>1.168746</td>\n",
       "      <td>0.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.158100</td>\n",
       "      <td>1.138648</td>\n",
       "      <td>0.735414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.139400</td>\n",
       "      <td>1.121623</td>\n",
       "      <td>0.735414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.165900</td>\n",
       "      <td>1.116052</td>\n",
       "      <td>0.735414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7370, training_loss=1.4356679235740466, metrics={'train_runtime': 264.1855, 'train_samples_per_second': 223.063, 'train_steps_per_second': 27.897, 'total_flos': 1.550610899278848e+16, 'train_loss': 1.4356679235740466, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"exp/{exp_name}/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 73.8%\n"
     ]
    }
   ],
   "source": [
    "# dataset[\"test\"]ã§è©•ä¾¡\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"text\"])\n",
    "# çµæœã®ãƒ©ãƒ™ãƒ«ã‚’ç•ªå·ã®ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "label2id = classifier.model.config.label2id\n",
    "predicted_labels = [label2id[result[\"label\"]] for result in results]\n",
    "\n",
    "# Accuracyã®è¨ˆç®—\n",
    "metrics = metric.compute(predictions=predicted_labels, references=dataset[\"test\"][\"label\"])\n",
    "\n",
    "print(f\"Test set accuracy: {metrics['accuracy']*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
