{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livedoor News Corpus のカテゴリ分類\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livedoor News Corpus の準備\n",
    "\n",
    "### Livedoor News Corpusとは\n",
    "\n",
    "[Livedoor News Corpus](https://www.rondhuit.com/download.html#news%20corpus)は、ライブドアニュースの記事を利用して作成されたテキストデータセットです。\n",
    "\n",
    "このデータには，以下の情報が含まれています．\n",
    "- 記事のURL\n",
    "- 記事の日付\n",
    "- 記事のタイトル\n",
    "- 記事の本文\n",
    "- 記事のカテゴリ\n",
    "\n",
    "記事のカテゴリは，\"トピックニュース\"，\"Sports Watch\"，\"ITライフハック\"，\"家電チャンネル\"，\"MOVIE ENTER\"，\"独女通信\"，\"エスマックス，\"livedoor HOMME\"，\"Peachy\"の9つからなります．\n",
    "\n",
    "記事は全部で7,367件あります．\n",
    "\n",
    "元々はテキストファイルとして公開されていますが，Hugging Face Datasetsに登録されているため，今回はこれを利用します．\n",
    "\n",
    "### データの読み込みと確認\n",
    "\n",
    "Hugging Face Datasetsを利用してデータを読み込みます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"shunk031/livedoor-news-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データは，`train`，`validation`，`test`の3つに分かれています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "# datasetのキーを表示する\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの中身を確認してみましょう:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 3,\n",
      " 'content': 'NHKの情報番組「お元気ですか日本列島」内の「ことばおじさんの気になることば」は、毎回、言葉の疑問に迫っていくコーナーだが、24日に放送された「日本に浸透している韓国語」の内容が、ネットユーザーの間で注目を集めている。  '\n",
      "            '放送によると、いま日本の若者の間では、携帯メールでハングルの絵文字を使うのがブームだと伝えている。「ハングルはかわいくてデザインにしやすい」と感じる人が増えているそうだ。また、若者へのインタビューでも「韓国語のほうが素直に言える。日本語だと恥ずかしい」「日本語では謝りにくいが『ミアネヨ、オンマ』（ごめんね、ママ）だと言いやすい」と答えており、実際にハングルを使ったメールも紹介された。  '\n",
      "            'これに対してネットユーザーは「そんな話聞いたことない」「こんなメール来たら縁を切るわ」など、番組が特集した“ブーム”の存在に疑問を呈する声が続出。また、「フジかと思ったらNHKかよ」「受信料払いたくない」「今度はNHKデモか?ww」など、NHKが韓国寄りの番組を放送していたことに批判的なネットユーザーの声も目立った。  '\n",
      "            '【関連情報】 ・「ことばおじさんの気になることば」  ・今、日本語にさりげなく韓国語を混ぜるのが大ブーム',\n",
      " 'date': '2011-11-25T19:48:00+0900',\n",
      " 'title': 'NHKの″韓流寄り″番組に批判の声',\n",
      " 'url': 'http://news.livedoor.com/article/detail/6062449/'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この結果から，データは\n",
    "- 'url' : 記事のURL\n",
    "- 'date' : 記事の日付\n",
    "- 'title' : 記事のタイトル\n",
    "- 'content' : 記事の本文\n",
    "- 'category' : 記事のカテゴリ\n",
    "で与えられることがわかりました．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train`, `valudation`, `test` それぞれでカテゴリの分布を確認してみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Counter({6: 900, 0: 870, 1: 870, 8: 870, 5: 842, 2: 772, 3: 770})\n",
      "validation: Counter({4: 511, 7: 134, 2: 92})\n",
      "test: Counter({7: 736})\n"
     ]
    }
   ],
   "source": [
    "# trainのcategoryの分布を計算する\n",
    "from collections import Counter\n",
    "\n",
    "for set_name in dataset.keys():\n",
    "    category_counter = Counter([data[\"category\"] for data in dataset[set_name]])\n",
    "    print(f\"{set_name}: {category_counter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このラベルのバランスでは適切な学習ができないため，改めてバランスをとりなおします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5893\n",
      "valid: 737\n",
      "test: 737\n"
     ]
    }
   ],
   "source": [
    "# datasetの中身を全て統合する\n",
    "all_data = []\n",
    "for set_name in dataset.keys():\n",
    "    all_data.extend(dataset[set_name])\n",
    "# シャッフルし, 80%をtrain, 10%をvalidation, 10%をtestにする\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(all_data)\n",
    "train_data = all_data[:int(0.8 * len(all_data))]\n",
    "valid_data = all_data[int(0.8 * len(all_data)):int(0.9 * len(all_data))]\n",
    "test_data = all_data[int(0.9 * len(all_data)):]\n",
    "print(f\"train: {len(train_data)}\")\n",
    "print(f\"valid: {len(valid_data)}\")\n",
    "print(f\"test: {len(test_data)}\")\n",
    "\n",
    "# 改めて，Datasets の形式にする\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_train = Dataset.from_list(train_data, features=dataset[\"train\"].features)\n",
    "dataset_valid = Dataset.from_list(valid_data, features=dataset[\"train\"].features)\n",
    "dataset_test = Dataset.from_list(test_data, features=dataset[\"train\"].features)\n",
    "dataset[\"train\"] = dataset_train\n",
    "dataset[\"validation\"] = dataset_valid\n",
    "dataset[\"test\"] = dataset_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Counter({6: 735, 0: 712, 7: 699, 8: 696, 2: 678, 5: 670, 1: 668, 3: 614, 4: 421})\n",
      "validation: Counter({1: 102, 8: 98, 7: 94, 2: 88, 5: 87, 6: 83, 3: 72, 0: 71, 4: 42})\n",
      "test: Counter({1: 100, 2: 98, 0: 87, 5: 85, 3: 84, 6: 82, 7: 77, 8: 76, 4: 48})\n"
     ]
    }
   ],
   "source": [
    "# 再度バランスを確認\n",
    "for set_name in dataset.keys():\n",
    "    category_counter = Counter([data[\"category\"] for data in dataset[set_name]])\n",
    "    print(f\"{set_name}: {category_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "データセットの内容を以下のように変更します:\n",
    "- 入力として利用するために，記事のタイトルと本文を連結し，'text'というキーで保存します．\n",
    "- 出力ラベルとして利用するために，カテゴリを'label'というキーで保存します．なお，カテゴリはすでに数値に変換されているため，そのまま利用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a1768f99804b7e90883019329b74b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac1f523e4ef4a0db256c78c7920610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7032f0dc425479a900d15d677dc005a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    return {\"text\": example[\"title\"] + \"：\" + example[\"content\"],\n",
    "            \"label\": example[\"category\"]}\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "適切に変換されたか確かめてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 6,\n",
      " 'content': '去る9月18日にボートの全日本選手権が行われ、女子シングルスカル決勝で若井江利が銀メダルを獲得した。  '\n",
      "            '巷ではそれほど名の知れた存在ではないが、若井は知る人ぞ知る美人アスリート。競技で日焼けした小麦色の肌に、さわやかな笑顔を振りまき、ボート界では随一の美人選手として知られている。  '\n",
      "            'スポーツタンクトップにサンバイザーをかける様は、これぞ健康美人という趣だ。  '\n",
      "            '若井はボートで有名な岐阜県の加茂高校出身。高校在学中に総体ダブルスカルで優勝し、ジュニア選手権日本代表でも活躍。進学した早稲田大でも数々のタイトルを獲得し、2006年のアジア大会では日本代表として銀メダルを手にした。  '\n",
      "            '現在は企業からスポンサードを受けながらフルタイムのボート選手として活動し、2010年アジア選手権では見事優勝を果たすなど、日本の女子ボート界を牽引するアスリートとして期待されている。  '\n",
      "            '今回の全日本選手権での2位という結果については、自身のブログで「一番嫌いな色のメダルですが、現実をしっかり受け止めて、次へ進みたいと思います」とコメント。目標とするロンドン五輪での表彰台へ向け、全速力で水面を駆ける。  '\n",
      "            '・若井江利フォトギャラリー',\n",
      " 'date': '2011-09-27T08:30:00+0900',\n",
      " 'label': 6,\n",
      " 'text': '【Sports '\n",
      "         'Watch】知る人ぞ知る美人アスリート、小麦肌の漕艇選手：去る9月18日にボートの全日本選手権が行われ、女子シングルスカル決勝で若井江利が銀メダルを獲得した。  '\n",
      "         '巷ではそれほど名の知れた存在ではないが、若井は知る人ぞ知る美人アスリート。競技で日焼けした小麦色の肌に、さわやかな笑顔を振りまき、ボート界では随一の美人選手として知られている。  '\n",
      "         'スポーツタンクトップにサンバイザーをかける様は、これぞ健康美人という趣だ。  '\n",
      "         '若井はボートで有名な岐阜県の加茂高校出身。高校在学中に総体ダブルスカルで優勝し、ジュニア選手権日本代表でも活躍。進学した早稲田大でも数々のタイトルを獲得し、2006年のアジア大会では日本代表として銀メダルを手にした。  '\n",
      "         '現在は企業からスポンサードを受けながらフルタイムのボート選手として活動し、2010年アジア選手権では見事優勝を果たすなど、日本の女子ボート界を牽引するアスリートとして期待されている。  '\n",
      "         '今回の全日本選手権での2位という結果については、自身のブログで「一番嫌いな色のメダルですが、現実をしっかり受け止めて、次へ進みたいと思います」とコメント。目標とするロンドン五輪での表彰台へ向け、全速力で水面を駆ける。  '\n",
      "         '・若井江利フォトギャラリー',\n",
      " 'title': '【Sports Watch】知る人ぞ知る美人アスリート、小麦肌の漕艇選手',\n",
      " 'url': 'http://news.livedoor.com/article/detail/5890429/'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルとトークナイザの準備\n",
    "\n",
    "モデルには，東北大学が提供している[日本語BERTモデル](https://huggingface.co/cl-tohoku/bert-base-japanese-v3)を利用します．\n",
    "\n",
    "### モデルとトークナイザの読み込み\n",
    "\n",
    "ラベル数が9であることに注意して，`AutoModelForSequenceClassification`と`AutoTokenizer`でモデルとトークナイザを読み込みます．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: i for i, label in enumerate(dataset[\"train\"].features[\"category\"].names)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "num_categories = 9\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                           num_labels=num_categories,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理（トークナイズ）\n",
    "\n",
    "学習をするために，データセットのテキストデータをトークナイズします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7ff85fe9dc60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3986c666dcf24708b3ba8718a6f816d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655fb0988cf443b5907802a63f0f8dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7e8f727696443fba5eecc00b9988f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['url',\n",
      " 'date',\n",
      " 'title',\n",
      " 'content',\n",
      " 'category',\n",
      " 'text',\n",
      " 'label',\n",
      " 'input_ids',\n",
      " 'token_type_ids',\n",
      " 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(tokenized_dataset[\"train\"][0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "長くなるので値の表示は割愛しますが，`input_id`，`attention_mask`，`token_type_ids`の3つのキーが追加されていることがわかります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "### 評価関数\n",
    "\n",
    "学習自体は`Trainer`によって損失関数が自動的に計算されて実行されますが，\n",
    "どの程度の性能が出ているかは別途評価する必要があります．\n",
    "そのための評価関数を用意しておきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習の設定\n",
    "\n",
    "学習の設定（`TrainingArguments`）を準備します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"base\"\n",
    "output_dir = f\"exp/{exp_name}/results\"\n",
    "logging_dir = f\"exp/{exp_name}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py311/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=10,            # 最大10エポックとする\n",
    "    per_device_train_batch_size=8,  # バッチサイズ\n",
    "    # auto_find_batch_size=True,    # バッチサイズを自動で見つける\n",
    "\n",
    "    weight_decay=0.01,              # 重み減衰\n",
    "    learning_rate=2e-5,             # 学習率\n",
    "    warmup_steps=500,               # ウォームアップステップ数\n",
    "\n",
    "    evaluation_strategy=\"epoch\",    # 評価はエポックごとに行う\n",
    "    metric_for_best_model=\"accuracy\", # 最良のモデルの評価指標\n",
    "    greater_is_better=True,           # 評価指標が大きいほど良い場合はTrue\n",
    "\n",
    "    output_dir=output_dir,          # モデルの保存先\n",
    "    save_strategy=\"epoch\",          # モデルの保存はエポックごとに行う\n",
    "    save_total_limit=3,             # 保存するモデルの数)\n",
    "\n",
    "    logging_dir=logging_dir,        # ログの保存先\n",
    "    logging_strategy=\"steps\",       # ログの保存はエポックごとに行う\n",
    "    logging_steps=100,              # 100ステップごとにログを出力する\n",
    "\n",
    "    load_best_model_at_end=True,    # 最良のモデルを最後にロードする\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 早期終了用のコールバック\n",
    "\n",
    "学習を途中で終了させるためのコールバックを用意します．\n",
    "ここでは，3エポック以上，Accuracyが向上しない場合に学習を終了させるように設定します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3, \n",
    "    early_stopping_threshold=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainerの作成\n",
    "\n",
    "学習を行うためのTrainerを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7370' max='7370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7370/7370 11:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.196954</td>\n",
       "      <td>0.945726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.149820</td>\n",
       "      <td>0.957938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>0.959294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.252548</td>\n",
       "      <td>0.960651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.193202</td>\n",
       "      <td>0.966079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.206794</td>\n",
       "      <td>0.967436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.203992</td>\n",
       "      <td>0.970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.215309</td>\n",
       "      <td>0.971506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.209539</td>\n",
       "      <td>0.972863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.972863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7370, training_loss=0.10265099366778051, metrics={'train_runtime': 677.6127, 'train_samples_per_second': 86.967, 'train_steps_per_second': 10.876, 'total_flos': 1.550610899278848e+16, 'train_loss': 0.10265099366778051, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"exp/{exp_name}/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストセットの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 97.3%\n"
     ]
    }
   ],
   "source": [
    "# dataset[\"test\"]で評価\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"text\"])\n",
    "# 結果のラベルを番号のリストに変換\n",
    "predicted_labels = [label2id[result[\"label\"]] for result in results]\n",
    "\n",
    "# Accuracyの計算\n",
    "metrics = metric.compute(predictions=predicted_labels, references=dataset[\"test\"][\"label\"])\n",
    "\n",
    "print(f\"Test set accuracy: {metrics['accuracy']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ヘッド部のみの学習\n",
    "\n",
    "前の例では，BERTの全体のパラメータをファインチューニングしました．\n",
    "過学習する可能性が高まるため，ヘッド部のみをチューニングするようにします．\n",
    "\n",
    "### モデルの再読み込み\n",
    "\n",
    "まず，モデルの読み直しを行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベースモデルパラメータのフリーズ\n",
    "\n",
    "次に，ベースモデルのパラメータをフリーズします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベースモデルのフリーズ\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainerの作成\n",
    "\n",
    "モデルの保存先のみを変更します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"head\"\n",
    "training_args.output_dir = f\"exp/{exp_name}/results\"\n",
    "training_args.logging_dir = f\"exp/{exp_name}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7370' max='7370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7370/7370 04:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.027200</td>\n",
       "      <td>1.954051</td>\n",
       "      <td>0.413840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.752400</td>\n",
       "      <td>1.691338</td>\n",
       "      <td>0.624152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.548100</td>\n",
       "      <td>1.506456</td>\n",
       "      <td>0.687924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.392200</td>\n",
       "      <td>1.371511</td>\n",
       "      <td>0.715061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.307200</td>\n",
       "      <td>1.278642</td>\n",
       "      <td>0.724559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.216000</td>\n",
       "      <td>1.214706</td>\n",
       "      <td>0.724559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.216100</td>\n",
       "      <td>1.168746</td>\n",
       "      <td>0.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.158100</td>\n",
       "      <td>1.138648</td>\n",
       "      <td>0.735414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.139400</td>\n",
       "      <td>1.121623</td>\n",
       "      <td>0.735414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.165900</td>\n",
       "      <td>1.116052</td>\n",
       "      <td>0.735414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7370, training_loss=1.4356679235740466, metrics={'train_runtime': 264.1855, 'train_samples_per_second': 223.063, 'train_steps_per_second': 27.897, 'total_flos': 1.550610899278848e+16, 'train_loss': 1.4356679235740466, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"exp/{exp_name}/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストセットの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 73.8%\n"
     ]
    }
   ],
   "source": [
    "# dataset[\"test\"]で評価\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "results = classifier(dataset[\"test\"][\"text\"])\n",
    "# 結果のラベルを番号のリストに変換\n",
    "label2id = classifier.model.config.label2id\n",
    "predicted_labels = [label2id[result[\"label\"]] for result in results]\n",
    "\n",
    "# Accuracyの計算\n",
    "metrics = metric.compute(predictions=predicted_labels, references=dataset[\"test\"][\"label\"])\n",
    "\n",
    "print(f\"Test set accuracy: {metrics['accuracy']*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
