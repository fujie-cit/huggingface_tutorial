{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私の名前は藤江と言いますか？\n",
      "^ w a [ t a sh i n o # n a [ m a e w a # f u [ j i e t o # i [ i m a ] s u k a ?\n"
     ]
    }
   ],
   "source": [
    "from espnet2.text.phoneme_tokenizer import pyopenjtalk_g2p_prosody\n",
    "\n",
    "text = \"私の名前は藤江と言いますか？\"\n",
    "phoneme_list = pyopenjtalk_g2p_prosody(text)\n",
    "# phoneme_list内の要素の \"N\" を \"nn\" に変換\n",
    "phoneme_list = [phoneme.replace(\"N\", \"nn\") for phoneme in phoneme_list]\n",
    "phonemes = \" \".join(phoneme_list)\n",
    "\n",
    "print(text)\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"sonoisa/t5-base-japanese\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'k a [ m a ] s u k a [] ]] ] a [] s u k a ['}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
    "generated = generator(phonemes)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"shunk031/livedoor-news-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def unicode_normalize(cls, s):\n",
    "    pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "    def norm(c):\n",
    "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "\n",
    "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "    s = re.sub('－', '-', s)\n",
    "    return s\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    s = re.sub('[ 　]+', ' ', s)\n",
    "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                      '\\u3040-\\u309F',  # HIRAGANA\n",
    "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                      ))\n",
    "    basic_latin = '\\u0000-\\u007F'\n",
    "\n",
    "    def remove_space_between(cls1, cls2, s):\n",
    "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "        while p.search(s):\n",
    "            s = p.sub(r'\\1\\2', s)\n",
    "        return s\n",
    "\n",
    "    s = remove_space_between(blocks, blocks, s)\n",
    "    s = remove_space_between(blocks, basic_latin, s)\n",
    "    s = remove_space_between(basic_latin, blocks, s)\n",
    "    return s\n",
    "\n",
    "def normalize_neologd(s):\n",
    "    s = s.strip()\n",
    "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
    "\n",
    "    def maketrans(f, t):\n",
    "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "\n",
    "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
    "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
    "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
    "    s = s.translate(\n",
    "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
    "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
    "\n",
    "    s = remove_extra_spaces(s)\n",
    "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
    "    s = re.sub('[’]', '\\'', s)\n",
    "    s = re.sub('[”]', '\"', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    text = example[\"title\"]\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.strip()\n",
    "    text = normalize_neologd(text)\n",
    "    text = text.lower()\n",
    "    phoneme_list = pyopenjtalk_g2p_prosody(text)\n",
    "    phoneme_list = [phoneme.replace(\"N\", \"nn\") for phoneme in phoneme_list]\n",
    "    phonemes = \" \".join(phoneme_list)\n",
    "    example[\"text\"] = text\n",
    "    example[\"phonemes\"] = phonemes\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd866231a3ca455487f83ce4a12d87bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5894 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4ed8c77e4344fe855cb053f50afa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b635315940324f608ed9a631be88fb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/736 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        text=examples[\"text\"],\n",
    "        max_length=model.config.max_length, # prob. 512\n",
    "        padding=\"max_length\",\n",
    "        truncation=True)\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"phonemes\"],\n",
    "        max_length=model.config.max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    examples.update(model_inputs)\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'date', 'title', 'content', 'category', 'text', 'phonemes', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"base\"\n",
    "output_dir = f\"exp/{exp_name}/results\"\n",
    "logging_dir = f\"exp/{exp_name}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py311/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=10,            # 最大10エポックとする\n",
    "    per_device_train_batch_size=8,  # バッチサイズ\n",
    "    auto_find_batch_size=True,    # バッチサイズを自動で見つける\n",
    "\n",
    "    weight_decay=0.01,              # 重み減衰\n",
    "    learning_rate=2e-5,             # 学習率\n",
    "    warmup_steps=500,               # ウォームアップステップ数\n",
    "\n",
    "    evaluation_strategy=\"epoch\",    # 評価はエポックごとに行う\n",
    "    # metric_for_best_model=\"accuracy\", # 最良のモデルの評価指標\n",
    "    # greater_is_better=True,           # 評価指標が大きいほど良い場合はTrue\n",
    "\n",
    "    output_dir=output_dir,          # モデルの保存先\n",
    "    save_strategy=\"epoch\",          # モデルの保存はエポックごとに行う\n",
    "    save_total_limit=3,             # 保存するモデルの数)\n",
    "\n",
    "    logging_dir=logging_dir,        # ログの保存先\n",
    "    logging_strategy=\"steps\",       # ログの保存はエポックごとに行う\n",
    "    logging_steps=100,              # 100ステップごとにログを出力する\n",
    "\n",
    "    load_best_model_at_end=True,    # 最良のモデルを最後にロードする\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3, \n",
    "    early_stopping_threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_training = False\n",
    "if enable_training:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_model_loading = True\n",
    "model_path = \"exp/base/results/checkpoint-58940\"\n",
    "if enable_model_loading:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: nhkの″韓流寄り″番組に批判の声\n",
      "target: ^ e [ n u e i ch i k e e n o _ k a [ nn ry u u y o r i _ b a [ nn g u m i n i # h i [ h a nn n o # k o ] e $\n",
      "predicted: ^ n i [ cl p o nn n o # sh i [ nn k o k u # sh i [ nn k o k u # sh i [ cl p o nn n o # sh i [ cl p o nn n o # sh i [ cl p o nn n o # sh i [ cl p o nn $\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "predicted = generator(dataset[\"train\"][i][\"text\"])\n",
    "print(f\"input: {dataset['train'][i]['text']}\")\n",
    "print(f\"target: {dataset['train'][i]['phonemes']}\")\n",
    "print(f\"predicted: {predicted[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^ e [ n u e i ch i k e e n o _ k a [ nn ry u u y o r i _ b a [ nn g u m i n i # h i [ h a nn n o # k o ] e $</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"私の名前は藤江と言いますか？\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> ^ sh i [ sh i ] sh i nn n o # sh i [ t a # sh i [ ts u # sh i [ t a # sh i [ t a # sh i [ t a # sh i [ t a # sh i [ t a # sh i [ t a $</s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model.generate(**inputs)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
