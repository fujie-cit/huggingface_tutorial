{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç§ã®åå‰ã¯è—¤æ±Ÿã¨è¨€ã„ã¾ã™ã‹ï¼Ÿ\n",
      "^ w a [ t a sh i n o # n a [ m a e w a # f u [ j i e t o # i [ i m a ] s u k a ?\n"
     ]
    }
   ],
   "source": [
    "from espnet2.text.phoneme_tokenizer import pyopenjtalk_g2p_prosody\n",
    "\n",
    "text = \"ç§ã®åå‰ã¯è—¤æ±Ÿã¨è¨€ã„ã¾ã™ã‹ï¼Ÿ\"\n",
    "phoneme_list = pyopenjtalk_g2p_prosody(text)\n",
    "# phoneme_listå†…ã®è¦ç´ ã® \"N\" ã‚’ \"nn\" ã«å¤‰æ›\n",
    "phoneme_list = [phoneme.replace(\"N\", \"nn\") for phoneme in phoneme_list]\n",
    "phonemes = \" \".join(phoneme_list)\n",
    "\n",
    "print(text)\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"sonoisa/t5-base-japanese\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'k a [ m a ] s u k a [] ]] ] a [] s u k a ['}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
    "generated = generator(phonemes)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"shunk031/livedoor-news-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja ã‹ã‚‰å¼•ç”¨ãƒ»ä¸€éƒ¨æ”¹å¤‰\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def unicode_normalize(cls, s):\n",
    "    pt = re.compile('([{}]+)'.format(cls))\n",
    "\n",
    "    def norm(c):\n",
    "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
    "\n",
    "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
    "    s = re.sub('ï¼', '-', s)\n",
    "    return s\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    s = re.sub('[ ã€€]+', ' ', s)\n",
    "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
    "                      '\\u3040-\\u309F',  # HIRAGANA\n",
    "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
    "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
    "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
    "                      ))\n",
    "    basic_latin = '\\u0000-\\u007F'\n",
    "\n",
    "    def remove_space_between(cls1, cls2, s):\n",
    "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
    "        while p.search(s):\n",
    "            s = p.sub(r'\\1\\2', s)\n",
    "        return s\n",
    "\n",
    "    s = remove_space_between(blocks, blocks, s)\n",
    "    s = remove_space_between(blocks, basic_latin, s)\n",
    "    s = remove_space_between(basic_latin, blocks, s)\n",
    "    return s\n",
    "\n",
    "def normalize_neologd(s):\n",
    "    s = s.strip()\n",
    "    s = unicode_normalize('ï¼-ï¼™ï¼¡-ï¼ºï½-ï½šï½¡-ï¾Ÿ', s)\n",
    "\n",
    "    def maketrans(f, t):\n",
    "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
    "\n",
    "    s = re.sub('[Ë—ÖŠâ€â€‘â€’â€“âƒâ»â‚‹âˆ’]+', '-', s)  # normalize hyphens\n",
    "    s = re.sub('[ï¹£ï¼ï½°â€”â€•â”€â”ãƒ¼]+', 'ãƒ¼', s)  # normalize choonpus\n",
    "    s = re.sub('[~âˆ¼âˆ¾ã€œã€°ï½]+', 'ã€œ', s)  # normalize tildes (modified by Isao Sonobe)\n",
    "    s = s.translate(\n",
    "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[Â¥]^_`{|}~ï½¡ï½¤ï½¥ï½¢ï½£',\n",
    "              'ï¼â€ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼Ÿï¼ ï¼»ï¿¥ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ã€œã€‚ã€ãƒ»ã€Œã€'))\n",
    "\n",
    "    s = remove_extra_spaces(s)\n",
    "    s = unicode_normalize('ï¼â€ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼Ÿï¼ ï¼»ï¿¥ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ã€œ', s)  # keep ï¼,ãƒ»,ã€Œ,ã€\n",
    "    s = re.sub('[â€™]', '\\'', s)\n",
    "    s = re.sub('[â€]', '\"', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    text = example[\"title\"]\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.strip()\n",
    "    text = normalize_neologd(text)\n",
    "    text = text.lower()\n",
    "    phoneme_list = pyopenjtalk_g2p_prosody(text)\n",
    "    phoneme_list = [phoneme.replace(\"N\", \"nn\") for phoneme in phoneme_list]\n",
    "    phonemes = \" \".join(phoneme_list)\n",
    "    example[\"text\"] = text\n",
    "    example[\"phonemes\"] = phonemes\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd866231a3ca455487f83ce4a12d87bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5894 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4ed8c77e4344fe855cb053f50afa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b635315940324f608ed9a631be88fb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/736 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        text=examples[\"text\"],\n",
    "        max_length=model.config.max_length, # prob. 512\n",
    "        padding=\"max_length\",\n",
    "        truncation=True)\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"phonemes\"],\n",
    "        max_length=model.config.max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    examples.update(model_inputs)\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'date', 'title', 'content', 'category', 'text', 'phonemes', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"base\"\n",
    "output_dir = f\"exp/{exp_name}/results\"\n",
    "logging_dir = f\"exp/{exp_name}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fujie/.conda/envs/py311/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=10,            # æœ€å¤§10ã‚¨ãƒãƒƒã‚¯ã¨ã™ã‚‹\n",
    "    per_device_train_batch_size=8,  # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    auto_find_batch_size=True,    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è‡ªå‹•ã§è¦‹ã¤ã‘ã‚‹\n",
    "\n",
    "    weight_decay=0.01,              # é‡ã¿æ¸›è¡°\n",
    "    learning_rate=2e-5,             # å­¦ç¿’ç‡\n",
    "    warmup_steps=500,               # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "\n",
    "    evaluation_strategy=\"epoch\",    # è©•ä¾¡ã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è¡Œã†\n",
    "    # metric_for_best_model=\"accuracy\", # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™\n",
    "    # greater_is_better=True,           # è©•ä¾¡æŒ‡æ¨™ãŒå¤§ãã„ã»ã©è‰¯ã„å ´åˆã¯True\n",
    "\n",
    "    output_dir=output_dir,          # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆ\n",
    "    save_strategy=\"epoch\",          # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è¡Œã†\n",
    "    save_total_limit=3,             # ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ•°)\n",
    "\n",
    "    logging_dir=logging_dir,        # ãƒ­ã‚°ã®ä¿å­˜å…ˆ\n",
    "    logging_strategy=\"steps\",       # ãƒ­ã‚°ã®ä¿å­˜ã¯ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«è¡Œã†\n",
    "    logging_steps=100,              # 100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹\n",
    "\n",
    "    load_best_model_at_end=True,    # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’æœ€å¾Œã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3, \n",
    "    early_stopping_threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_training = False\n",
    "if enable_training:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_model_loading = True\n",
    "model_path = \"exp/base/results/checkpoint-58940\"\n",
    "if enable_model_loading:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: nhkã®â€³éŸ“æµå¯„ã‚Šâ€³ç•ªçµ„ã«æ‰¹åˆ¤ã®å£°\n",
      "target: ^ e [ n u e i ch i k e e n o _ k a [ nn ry u u y o r i _ b a [ nn g u m i n i # h i [ h a nn n o # k o ] e $\n",
      "predicted: ^ n i [ cl p o nn n o # sh i [ nn k o k u # sh i [ nn k o k u # sh i [ cl p o nn n o # sh i [ cl p o nn n o # sh i [ cl p o nn n o # sh i [ cl p o nn $\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "predicted = generator(dataset[\"train\"][i][\"text\"])\n",
    "print(f\"input: {dataset['train'][i]['text']}\")\n",
    "print(f\"target: {dataset['train'][i]['phonemes']}\")\n",
    "print(f\"predicted: {predicted[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^ e [ n u e i ch i k e e n o _ k a [ nn ry u u y o r i _ b a [ nn g u m i n i # h i [ h a nn n o # k o ] e $</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"ç§ã®åå‰ã¯è—¤æ±Ÿã¨è¨€ã„ã¾ã™ã‹ï¼Ÿ\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> ^ sh i [ sh i ] sh i nn n o # sh i [ t a # sh i [ ts u # sh i [ t a # sh i [ t a # sh i [ t a # sh i [ t a # sh i [ t a # sh i [ t a $</s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model.generate(**inputs)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
