{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quicktour\n",
    "\n",
    "PEFTは，大きな事前学習済みモデルのファインチューニングをする，パラメタ効率の良い方法を提供します．\n",
    "従来のパラダイムでは，各ダウンストリームタスクのためにモデルの全てのパラメタをファインチューニングしていましたが，非常にコストがかかり，今日のモデルは莫大な数のパラメタを持つため現実的な方法ではありません．\n",
    "そのかわり，より少量のプロンプトパラメタを学習したり，low-rank adaptation（LoRA）のような追加パラメタを利用することで，学習するパラメタの量を減らすことをします．\n",
    "\n",
    "このクイックツアーでは，PEFTの主な機能や典型的なコンシューマデバイスではアクセスできないような大きなモデルでの学習や推論の方法を紹介します．\n",
    "\n",
    "## 学習（Train）\n",
    "\n",
    "各PEFT手法は，`PeftModel`を構築するための全ての重要なパラメタが保存されている`PeftConfig`クラスによって定義されます．\n",
    "例えば，LoRAで学習するには，`LoraConfig`クラスを読み込み作成し，次に示すパラメタを指定します:\n",
    "\n",
    "- `task_type`: 学習するタスク（ここではsequence-to-sequence言語モデルを扱います）\n",
    "- `inference_mode`: 推論で使うかどうか\n",
    "- `r`: 低次元行列（low-rank matrices）の次元\n",
    "- `lora_alpha`: 低次元行列のスケーリング係数\n",
    "- `lora_dropout`: LoRA層のドロップアウト確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [LoraConfig]()リファレンスで，他に調整可能なパラメタ（例えば，対象とするモジュール，バイアスのタイプなど）を確認してください．\n",
    "\n",
    "`LoraConfig`を設定したら`PeftModel`を`get_peft_model()`で作成します．\n",
    "この実行には，Transformersライブラリで読み込めるベースモデルと，LoRAで学習するためにモデルをどのように設定するかといったパラメタを含む`LoraConfig`が必要です．\n",
    "\n",
    "ファインチューンするためのモデルを読み込みます:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベースモデルと`peft_config`を`get_peft_config()`関数を使ってラップし，`PeftModel`を作成します．\n",
    "学習可能なパラメタの数の感覚を得るために，`print_trainable_params()`を使います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 1,231,940,608 || trainable%: 0.1915\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベースの`bigscience/mt0-large`のパラメタ数1.2Bに対して，0.19%のパラメタのみを学習対象としています！\n",
    "\n",
    "これで，Transformersの`Trainer`，`Accelerate`やカスタムPyTorch学習ループなどを使ってモデルを学習することができます．\n",
    "\n",
    "例えば，`Trainer`クラスを使って学習する場合，`TrainingArguments`クラスをいくつかの学習ハイパーパラメータで設定します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./your-name/bigscience/mt0-large-lora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル，学習パラメタ，データセット，トークナイザなど，必要なものを`Trainer`に私，`train()`を呼び出し学習を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fujie/anaconda3/envs/py311/lib/python3.11/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: training requires a train_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   1811\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py:835\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03mReturns the training [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;124;03mSubclass and override this method if you want to inject some custom behavior.\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 835\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: training requires a train_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    837\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset\n\u001b[1;32m    838\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: training requires a train_dataset."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # train_dataset=dataset[\"train\"], \n",
    "    # eval_dataset=dataset[\"validation\"],\n",
    "    # tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの保存\n",
    "\n",
    "学習が終わったら，`save_pretrained()`関数を使って適当なディレクトリにモデルを保存します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"output_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`push_to_hub()`関数でHubに保存することもできます（先にHugging Faceアカウントでログインする必要があります）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f1ccc6c836413c826f1482e7808ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# model.push_to_hub(\"your-name/bigscience/mt0-large-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "何の方法も，学習された追加のPEFTの重みのみを保存します．つまり，保存，転送，読み込みを効率よく行うことができます．\n",
    "例えば，LoRAで学習された`facebook/opt-350m`モデルは，`adapter_config.json`と`adapter_model.safetensors`の二つのファイルしか持ちません．\n",
    "`adapter_model.safetensors`のファイルサイズはたったの6.3MBしかありません．\n",
    "ベースモデルのパラメタは700MB程度のサイズです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論（Inference）\n",
    "\n",
    "> `AutoPeftModel`クラスの利用可能な全リストを確認するには[AutoPeftModel](https://huggingface.co/docs/peft/package_reference/auto_class)APIリファレンスを参照してください．\n",
    "\n",
    "PEFTで学習されたモデルを推論に使うためには，`AutoPeftModel`クラスの`from_pretrained()`関数を使ってモデルを読み込みます:\n",
    "```python\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"ybelkada/opt-350m-lora\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_length=50)\n",
    "print(tokenizer.decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\n",
    "```\n",
    "\n",
    "`AutoPeftModelFor`クラスで明示的にサポートされていないその他のタスク，例えば音声認識など，の場合，ベースの`AutoPeftModel`クラスを使ってモデルを読み込むことができます．\n",
    "```python\n",
    "from peft import AutoPeftModel\n",
    "\n",
    "model = AutoPeftModel.from_pretrained(\"smangrul/openai-whisper-large-v2-LORA-colab\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次のステップ\n",
    "\n",
    "PEFTを用いた一つの学習方法について学びました．\n",
    "プロンプトチューニングなどのその他の方法を試してみることをお勧めします．\n",
    "手順はこのクイックツアーで見たものと非常に似ています:\n",
    "\n",
    "1. PEFTメソッドのために`PeftConfig`を設定します．\n",
    "2. 設定とベースモデルから，`get_peft_model()`を使って`PeftModel`を作成します．\n",
    "\n",
    "あとは好きなように学習をするだけです！\n",
    "PEFTモデルを推論のために読み込むのには，`AutoPeftModel`クラスを使えます．\n",
    "\n",
    "意味セグメンテーション（semantic segmentation），多言語音声認識（multilingual speech recognition），DreamBooth，トークン識別などなど，特定のタスクのためのPEFTメソッドでモデルを学習することに興味があれば，タスクガイドを見てください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
